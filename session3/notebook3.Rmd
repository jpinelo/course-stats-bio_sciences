---
title: "Introduction to Statistics for Biosciencies"
output: html_notebook
---


###### Course **CIIMAR**
###### Lecturer **Aldo Felpeto**
###### 11-14 Jan 2021   
### **13 Jan - Session 3**


## LINEAR REGRESSION

* **Independent** variable = **Predictor** (traditionally denoted x and plotted on this axis)   
* **Dependent** variable = **Response** (y)   

Linear model:    

> *y = ax +b*   

Where:   
* a = slope   
* b = intercept   (value of response when predictor is 0 - so a sort of starting reference)

Fitting the model to the data consists in finding the best estimates for a and b.         
Finding the estimates can be done through several methods. Here we use the **Minimization of the Residual Square Sum**.      

#### Minimization of the Residual Square Sum   
* Residuals are the magnitude of the vector between each real point to the model fitted (vertical straight line between each point and the model).
* The residual variance is what is left; what cannot be explained by the model.   

Residuals are represented by the dashed lines in the figure below.   
![Illustration of the residuals as dashed lines.](residuals.png)    
The minimized value is found by **iterating** a matrix algorithm that calculates the residuals for different fitted values of y calculated from iterated values of the two parameters of the linear function (a and b).   
An important property of the residuals is that their **sum is always equal to 0**.   


#### Example of Linear Fit, or Linear Regression        
```{r}
data(iris)
str(iris)
```


#### Look at all pairwise combinations        
```{r}
pairs(iris[,-5], lower.panel=NULL, col=as.numeric(iris$Species))
````
In this example, it looks like petal length and petal with have a strong and **linear** association.

To create a model we use `lm()`, which takes the dependent and independent variables.   
`lm(data= data, dependent_var~independent_var)` OR `lm(data$dependent_var~data$independent_var)`   
The formulation can be read as:      

> *Dependent variable* **as predicted by** *the Independent variable*   

> *Dependent variable* **as a function of** *the Independent variable*   

####     
```{r}
model <- lm(data= iris, Petal.Width~Petal.Length)
````
The formulation above means: *Petal width predicted by petal length*


###### See model data        
```{r}
summary(model)
````

##### Interpreting the model:   
**Residuals**   
* Residual - the median shall be approx. 0. because on a normal distrib. the mean/median is 0, and the quartiles shalll be simmilar.
Most importantly the **Coefficients**:
* Pr is the p value for the t student's test   
These tests are against 0 because if the parameter was 0 it would not be useful for the model. In this case parameters are significantly diff from 0.   
**Residual Standar Error**   
* The residual standard error is the value that is minimized by iteration   
* The residual standard error is not the sum of the residuals, that, is always = 0   
* This residual standard error is interpreted as the unexplained variance   
* Because the residual standard error is calculated as the sum of all the squared differences between a real value and the expected theoretical value (remember the formula of the variance), it is essentially the same thing.
* Regarding degrees of freedom, they have interpretations expressed in a different way, depending if we are in a context of physics or statistics, but in essence, they are equivalent.   
**F Statistic** (refers to the whole model)     
* For large n this tends to be significant, so be careful with interpreting it. So it basically says whether there is enough data in the model. Not the performance of the model (how much is explained by it).       
**Degrees of Freedom** (DF) in general       
* they are ;ike dimension, or vars with which the model can play to find best fit
* Nomarally = n-1   
**Degrees of Freedom** (DF) interpreting in the summary of the model   
= n - 2   (We could say that we have 149 (150 data – 1) degrees of freedom for the error and 1 degree of freedom for the model (2 parameters – 1). The total degrees of freedom are hence 148)     
**Multiple R-squared** (refers to whole model)   
* Coeficient of determination (this is not pearson corr. coef.) _ gives the percentage of variance of data explained by the model. In this case the model explains 93% of the varience. This is only valid for linear regressions.
**Adjusted R-squared** (refers to whole model)    
Gives the percentage of variance of data explained by the model. It if useful for multiple regression. When the model has several variables, it is usually better, but might not be proportional to the use of so many variables. The model might be apparently better spuriously. So this is a **penalized R-squared** for using many vars, which should increase the quality of the model significantly but might not.   
We look for the mix of vars that minimizes the difference between Multiple and Adjusted-R-squared values. There are functions to do this automatically.    


#### Assumptions of Linear Regression:   
* Linear relationship   
* Residuals should be independent
* Residuals shall be normal   


### complete...






### Testing for the assumptions

for n > 1000 just check visually for normallity since the tests are not reliable   

```{r}

````

####     
```{r}

````







![Ideally, the ted line should be horizontal.](spread-level.png)  
Ideally, the red line should be horizontal, flat. That will mean that residuals ar constant and residuals are normal.    
not  flat red line means that residuals are auto-correlated  
Means residuals are increasing and they should be +- constant    
If residuals show a trend, such as here, it means that there is a var that is explaining it, which we should include. In this example, including the species could solve some of it. However this is a qualitative var so another model is needed.    



### To transform one var, we should no assume that log will do it.
`boxcox()` finds the optimal lambda for the normality of our data (the residuals of the model), or the optimal transformation function. If we have a good normal, it is very likely that other parameters are also resolved.
* Transform the variable then redo the model with transformed var.     

#### Nother option for trnaforming   
Here applies to the variable   
`require(bestNormalize)`
`bestNormalize(iris$Petal.Width)`    
choose the transformation based on output of above (look for lower value)
transform then redo model
(to transform use a function that has the exact name of the transform chosen on the output of the `bestNormalize()` )


## NON-LINEAR REGRESSION

#### Assumptions:     







![Hiperbolic.](hiperbolic.png)


### Saturation constant   
![Type IV.](type_IV.png)

```{r}
dat<-read.csv2("RotiferNR.csv",sep=";")
head(dat)
````
```{r}
plot(dat$FoodN,dat$rN,xlab="Food abundance",ylab="Daily growth rate",pch=19)
````

#### Non-linear sum of squares
`nls()`    

```{r}
typ2<- nls(rN~(rmaxN*FoodN/(KfN+FoodN)),start=list(rmaxN=0.9,KfN=20000),data= dat)
```
rmax = max y (max growth rate)
KfN = the value of x when y has half of the maximum of y (this can be approximate; in the order of magnitude of the value)
     
     
```{r}
summary(typ2)
````
Result means result not significant.   


So try another model
####     
```{r}
typ4<- nls(rN~(rmaxIVN*FoodN)/((FoodN+KfIVN)*(1+(iN*FoodN))),start=list(rmaxIVN=0.9,iN=0.000003769,KfIVN=2000),data=dat)
```
rmax = max y (max growth rate)
KfIVN = the value of x when y has half of the maximum of y (this can be approximate; in the order of magnitude of the value)
these values above should be the same as int he example above ty2
iN = slope of the decrease of the down-slope of the curve



####     
```{r}
summary(typ4)
````

Results mean this model is good fit.   


Good model:   
* as simple as possible (less parameters);   
* explains the variance as much of possible.   





```{r}
library(qpcR)
```

```{r}
AICc(typ2)
```

```{r}
AICc(typ4)
```

AIC - the lowest the better (including negative), so typ4 much better
This AIC balances things out considering the number of parameters used. If use more parameters but results not much better, then AIC is higher.   
It works similar to the Adjusted_R-squared       

If we get a small difference between the IAC of two models, choose the simpler model.   
If the values are similar, then can test for the significance of the difference.   
There is only point in testing if the most complex has lower AIC, otherwise the choice is obviously the simpler model.   

```{r}
anova(typ2,typ4)
````

The values are indeed statistically significantly different.   
We can only test this is the **models are nested**. Meaning that one model is a specific case of the other one. For example add another var to one model. The second model is a specific case of the first and they are nested.      

####     
```{r}
typeII<-curve((1.97*(x-(-39)))/(974+x),from=0,to=800000)
```

####     
```{r}
typeIV<-curve((4.1*(x-(- 664)))/((31950+x)*(1+(0.00000254*x))),from=0,to=800000)
```

####     
```{r}
plot(dat$FoodN,dat$rN,xlab="Food abundance",ylab="Daily growth rate",pch=19)
points(typeII,type="l",lty=2)
points(typeIV,type="l")
legend("bottomright",c("type II","type IV"),lty=c(2,1))
```
Residuals are not homogeneous.   
How to justify lack of homogeinity. It is a technical problem. It comes from the fact that on data collection it is very difficualt to measure the aount of food, and we should take them as estimates. Furthermore, the slope is huge for small x, so it is too hard to know the exact y for eaaach x.   

Here we could transform the var, but then we will not understand how the food intake affects the population.   
So we could not interpret and use the results.   
In general we do not transform variables in non-linear regressions.   

####     
```{r}

```

####     
```{r}

```
complete...

#### simulated annealing   

####     
```{r}

```





## NON-PARAMETRIC REGRESSION OVERVIEW   
The tern non-parametric is used differently then when used in the context of testing for meand or variance
There are parameters, but they are tailored.   

# EXERCISES   

## 1

With the data from the file “Data_exercise1.csv”, fit a simple linear regression model with lm(). Then check the residuals plot and plot the real data versus the fitted by the model.
After you do all these things, the teacher will show you how to improve this fit with a non-linear regression model.


####     
```{r}
data <- read.csv("Data_exercise1.csv")
head(data)
```

####     
```{r}
str(data)
```

####     
```{r}
model <- lm(Y~X, data = data)
```

####     
```{r}
summary(model)
```

####     
```{r}
plot(model)
```
Conclusion model very bad...



####     
```{r}
plot(data$X, data$Y)
```
As we can see above hte relationship is not linear, this is why the linear model did not work out well.   


## 2   
Fit a Monod model (you can google to find the equation) to the data from the file “Growth.csv”. Our dependent variable would be the growth rate.
Plot the data and overlay the model fit Suggestion: functions nls() for the fit and curve() for overlaying the model fit in the plot.

```{r}
data1 <- read.csv("Growth.csv", dec = ",")
data1
```
```{r}
plot(data1$NO2, data1$r)
```


Dependent variable = y = r = Growth rate

```{r}
model2 <- nls(r ~ (rmax * NO2/(K + NO2)), start = list(rmax = 0.7, K = 5), data = data1)
```

```{r}
summary(model2)
```

```{r}
monod <- curve((0.68 * x)/(2.37 + x),from = 0, to = 350)
plot(data1$NO2, data$r)
points(monod, type = "l", lty = 2)
```

```{r}


```































